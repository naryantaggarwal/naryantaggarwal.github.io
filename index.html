<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Naryan Aggarwal's Portfolio</title>
    <!-- Add CSS links here -->
	<link rel="stylesheet" href="styles.css">
	<script>
		function showTab(tabId) {
			// Hide all tab content
			var contents = document.getElementsByClassName('tab-content');
			for (var i = 0; i < contents.length; i++) {
				contents[i].classList.remove('active');
			}

			// Deactivate all tab buttons
			var buttons = document.getElementsByClassName('tab-button');
			for (var i = 0; i < buttons.length; i++) {
				buttons[i].classList.remove('active');
			}

			// Show the selected tab content and activate the corresponding button
			document.getElementById(tabId).classList.add('active');
			document.querySelector(`[onclick="showTab('${tabId}')"]`).classList.add('active');
		}
	</script>
</head>

<body>

	<div class="background-wrapper">
		<nav id="navbar">
			<ul>
				<li><a href="#about-anchor">About Me</a></li>
				<li><a href="#skills-section">Skills</a></li>
				<li><a href="#education">Education</a></li>
				<li><a href="#experience">Experience</a></li>
				<li><a href="#projects">Projects</a></li>
				<li><a href="https://www.linkedin.com/in/naryana/">LinkedIn</a></li>
				<li><a href="https://github.com/naryantaggarwal">GitHub</a></li>
				<li><a href="#contact">Contact</a></li>
			</ul>
		</nav>
		<div class="content-container">	
			<!-- Header Section -->
			<header>
				<a id="about-anchor"></a>
				<div class="container">
					<h1>Naryan Aggarwal</h1>
					<p>Data Analyst</p>
				</div>
			</header>
		</div>
		
		<div class="content-container">	
			<!-- About Section -->
			<section id="about">
				<div class="container">
					<h2>About Me</h2>
					<p>As a graduate with a Bachelor's degree in Business Analytics, I am highly motivated 
						to explore opportunities that apply my analytical and business skills to data-driven decision-making. 
						With a solid foundation in statistical analysis, data visualization, and Python, I am passionate about 
						investing in opportunities with both small businesses and large corporations. My goal is to extract 
						insights from complex datasets to drive innovation and efficiency. I am excited to embark on a career 
						path where I can make an impact by transforming data into valuable solutions..</p>
				</div>
			</section>
		</div>
		
		
		<div class="content-container">
			<!-- Experience Section -->
			<section id="experience">
				<div class="container">
					<h2>Work Experience</h2>

					<div class="experience-item">
						<h3>Intake Specialist</h3>
						<p>Benedictine University, Lisle, IL | Sept 2024 – May 2025</p>
						<ul>
							<li>Coordinated communication between external parties and internal departments to support admissions workflows.</li>
							<li>Interpreted and navigated reports, transcripts, and information requests to provide accurate solutions for prospective students.</li>
							<li>Fostered seamless connections across the admissions team and university leadership.</li>
							<li>Delivered campus walkthroughs with clarity and consistency, often serving as the initial point of contact for new visitors and prospects.</li>
						</ul>
					</div>

					<div class="experience-item">
						<h3>Animator/Owner (Part-Time)</h3>
						<p>Nyr Animation Studio, Plainfield, IL | January 2023 – Present</p>
						<ul>
							<li>Worked for over 30 commercial and private clients utilizing several tools like Blender and Cinema4D.</li>
							<li>Produced 60+ models and animations according to custom specifications and changing demand.</li>
							<li>Maintained ongoing communication with at least 3 clients simultaneously on average to manage evolving project requirements and deliver high-quality assets on time.</li>
						</ul>
					</div>

					<div class="experience-item">
						<h3>Development Intern</h3>
						<p>Moksha Data, Houston, TX | June 2022 – August 2022</p>
						<ul>
							<li>Engineered phonetic sorting algorithms to streamline processing of over 1TB of data per unit of time.</li>
							<li>Audited and optimized a 15-step data pipeline using Python, AWS, and Redshift to improve processing efficiency.</li>
							<li>Constructed high-speed links between SQL databases, enabling rapid processing of big data.</li>
						</ul>
					</div>

					<div class="experience-item">
						<h3>Marketing Communications Manager</h3>
						<p>CultureU, Bloomington, IN | September 2019 – May 2020</p>
						<ul>
							<li>Lead a team and collaborated directly with executive leadership to implement three company-wide marketing initiatives.</li>
							<li>Directed the development of the startup’s brand identity and led the launch of its initial marketing campaign.</li>
							<li>Evaluated and approved product pricing and promotional strategies prior to rollout.</li>
						</ul>
					</div>

					<div class="experience-item">
						<h3>Beta Tester & Game Design Consultant</h3>
						<p>Freelance Worker, Lombard, IL | January 2014 – July 2019</p>
						<ul>
							<li>Analyzed 30+ games to assess engagement, market potential, and design flaws.</li>
							<li>Advised development teams on aligning product features with current trends and target audiences.</li>
							<li>Partnered with studios including Creative Assembly and Bethesda, as well as several indie developers.</li>
						</ul>
					</div>
				</div>
			</section>
		</div>

		<div class="content-container">
			<!-- Education Section -->
			<section id="education">
				<div class="container">
					<h2>Education</h2>
					<div class="education-item">
						<h4>Bachelors of Business in Business Analytics – GPA: 3.5</h3>
						<p>Benedictine University, Lisle, IL | May 2025</p>
					</div>
					<p>See more on <a href="https://www.linkedin.com/in/naryana/">LinkedIn</a>.</p>
				</div>
			</section>
		</div>
		
		<div class="content-container">
			<!-- Projects Section with Tabs -->
			<section id="projects">
				<div class="container">
					<h2>Projects</h2>

					<!-- Tab Navigation -->
					<div class="tabs">
						<button class="tab-button active" onclick="showTab('tab1')">Most Recent Projects</button>
						<button class="tab-button" onclick="showTab('tab2')">Work Projects</button>
						<button class="tab-button" onclick="showTab('tab3')">Academic Projects</button>
						<button class="tab-button" onclick="showTab('tab4')">Personal Projects</button>
					</div>

					<!-- Tab Content -->
					<div id="tab1" class="tab-content active">
						<!-- Current Projects -->
						<!-- Enrollment Data Analysis -->
						<div class="project-item">
							<h3>Enrollment Data Analysis (Current)</h3>
							<ul>
								<li>Analyzing complex enrollment datasets by extracting, transforming, and visualizing data in Tableau to identify key trends across demographics, retention rates, and academic programs across 5 years.</li>
								<li>Delivered a comprehensive performance review of enrollment strategies, highlighting data-driven insights and pinpointing areas for improvement</li>
								<li>Identified and addressed a 30% drop in student retention, presenting actionable insights that contributed to measurable recovery efforts.</li>
								<li>Recommended strategic actions to enhance retention and program effectiveness, influencing executive decision-making, and increase ROI, as well as increase student capture by 40%.</li>
								<li>Integrating demographic, labor market, and tuition perception data to align recommendations with Chicago-area student expectations.</li>
								<li>Presenting findings to the University President and Board of Trustees, demonstrating high-level communication and stakeholder engagement.</li>
							</ul>
						</div>
						
						<!-- Clinical Website Text Analysis and Update -->
						<div class="project-item">
							<h3>Clinical Website Text Abalysis and Update (Freelance) (Current)</h3>
							<ul>
								<li>Processing data for 4,500+ U.S. cities using Python, Excel, and Shell scripting, sourcing up-to-date statistics from the CDC.</li>
								<li>Developing automated workflows to clean, transform, and analyze public health data, ensuring accuracy and consistency across all entries.</li>
								<li>Created distinct, human-readable summaries for each city’s statistics to support tailored content delivery.</li>
								<li>Collaborated with business stakeholders and technical teams to align data outputs with project goals, ensuring both technical integrity and business usability.</li>
							</ul>
						</div>
						
						<!-- Health Clinic Data Analysis -->
						<div class="project-item">
							<h3>Health Clinic Data Analysis</h3>
							<ul>
								<li>Used Python for natural language processing and analysis on over 20,000 records of clinics nationwide.</li>
								<li>Created visualizations and usable information on city, state, and national levels.</li>
								<li>Performed large-scale de-duplication to enhance data accuracy and relevance.</li>
							</ul>
						</div>
					</div>

					<div id="tab2" class="tab-content">
						<!-- Work Projects -->
						<!-- Enrollment Data Analysis -->
						<div class="project-item">
							<h3>Enrollment Data Analysis</h3>
							<ul>
								<li>Analyzing complex enrollment datasets by extracting, transforming, and visualizing data in Tableau to identify key trends across demographics, retention rates, and academic programs across 5 years.</li>
								<li>Delivered a comprehensive performance review of enrollment strategies, highlighting data-driven insights and pinpointing areas for improvement</li>
								<li>Identified and addressed a 30% drop in student retention, presenting actionable insights that contributed to measurable recovery efforts.</li>
								<li>Recommended strategic actions to enhance retention and program effectiveness, influencing executive decision-making, and increase ROI, as well as increase student capture by 40%.</li>
								<li>Integrating demographic, labor market, and tuition perception data to align recommendations with Chicago-area student expectations.</li>
								<li>Presenting findings to the University President and Board of Trustees, demonstrating high-level communication and stakeholder engagement.</li>
							</ul>
						</div>

						<!-- Clinical Website Text Analysis and Update -->
						<div class="project-item">
							<h3>Clinical Website Text Abalysis and Update (Freelance)</h3>
							<ul>
								<li>Processing data for 4,500+ U.S. cities using Python, Excel, and Shell scripting, sourcing up-to-date statistics from the CDC.</li>
								<li>Developing automated workflows to clean, transform, and analyze public health data, ensuring accuracy and consistency across all entries.</li>
								<li>Created distinct, human-readable summaries for each city’s statistics to support tailored content delivery.</li>
								<li>Collaborated with business stakeholders and technical teams to align data outputs with project goals, ensuring both technical integrity and business usability.</li>
							</ul>
						</div>
						
						<!-- Health Clinic Data Analysis -->
						<div class="project-item">
							<h3>Health Clinic Data Analysis</h3>
							<ul>
								<li>Used Python for natural language processing and analysis on over 20,000 records of clinics nationwide.</li>
								<li>Created visualizations and usable information on city, state, and national levels.</li>
								<li>Performed large-scale de-duplication to enhance data accuracy and relevance.</li>
							</ul>
						</div>
						
						<!-- Network Backup Program -->
						<div class="project-item">
							<h3>Network Backup Program</h3>
							<ul>
								<li>Developed using Bash and shell scripting, allowing the backup of up to 10 systems into an organized and encrypted file.</li>
								<li>Supports Linux, MacOS, Windows, and mixed environments, with automation timers for regular backups.</li>
							</ul>
						</div>

						<!-- MySQL Database Log Analyzer -->
						<div class="project-item">
							<h3>MySQL Database Log Analyzer</h3>
							<ul>
								<li>Analyzes MySQL logs to generate reports on new IPs, errors, login frequencies, and more.</li>
								<li>Improves speed and efficiency of reviewing backend performance across multiple databases.</li>
							</ul>
						</div>

						<!-- Process Monitor -->
						<div class="project-item">
							<h3>Process Monitor</h3>
							<ul>
								<li>Monitors specified processes for memory and storage usage, CPU and network performance, and detects anomalous behavior.</li>
							</ul>
						</div>

						<!-- System Info Reporter -->
						<div class="project-item">
							<h3>System Info Reporter</h3>
							<ul>
								<li>Generates health and performance reports for the hosted system, suitable for integration with complex analyzers.</li>
							</ul>
						</div>
						
					</div>

					<div id="tab3" class="tab-content">
						<!-- Academic Projects -->
						<!-- ARIMA Models -->
						<div class="project-item">
							<h3>ARIMA Models</h3>
							<ul>
								<li>Developed ARIMA (AutoRegressive Integrated Moving Average) models using Python to forecast future values in time series data. This approach was employed to analyze historical data and generate predictions, focusing on economic indicators and other trend-sensitive datasets.</li>
								<pre><code class="language-python">
from pandas import DataFrame, read_excel, concat
from matplotlib import pyplot as plt
from statsmodels.tsa.api import acf, pacf, ARIMA, arma_order_select_ic

def correlogramAsDataFrame(correlogram, partial=False):
	# Find the correlogram with confidence intervals for each lag
	if partial:
		label="PACF"
	else:
		label="ACF"
	vals, confints = correlogram
	# Separate the lower bounds and upper bounds of the confidence intervals
	lower = confints.take(indices=0, axis=1)
	upper = confints.take(indices=1, axis=1)

	# Print the correlogram in text form for preciser reading
	return DataFrame({label: vals, "Lower": lower, "Upper": upper})

def plotCorrelogram(correlogram):
	# Plot the correlogram with the confidence intervals for each lag
	plt.plot(correlogram.iloc[:,0], color="gray", label=correlogram.columns[0])
	plt.plot(correlogram[["Lower"]], color="black", linestyle="dashed", label="Lower")
	plt.plot(correlogram[["Upper"]], color="black", linestyle="dotted", label="Upper")
	plt.legend()
	plt.show()

def diffSeries(series):
	return [series[i+1]-series[i] for i in range(len(series) - 1)]

# Access the data set; `sheet_name=None` means import all sheets from the workbook.
data=read_excel("https://www.census.gov/construction/nrc/xls/starts_cust.xlsx", sheet_name=None, header=[5, 5, 5, 5])

# Extract rows 0 to 779 and columns 0 to 1 from the "Seasonally Adjusted" sheet.
df = data["Seasonally Adjusted"].iloc[0:780, 0:2]
# Update the column headers
df.columns = ["Month", "PHS"]
# Get the phs `Series` by itself
phs = df["PHS"]
# Label the rows of `phs` by the month
phs.index = df["Month"]

# Do an initial exploratory plot
plt.plot(phs)
plt.show()

# Find and display the autocorrelation function.
correlAcf = acf(phs, nlags=10, alpha=0.05)
acfDF = correlogramAsDataFrame(correlAcf)
plotCorrelogram(acfDF)

# Find and display the partial autocorrelation function.
correlPacf = pacf(phs, nlags=10, alpha=0.05)
pacfDF = correlogramAsDataFrame(correlPacf, partial=True)
plotCorrelogram(pacfDF)

# Order selection
p = 1
d = 1
q = 2
model = ARIMA(phs, order=(p, d, q))
fitted_model = model.fit()
print(fitted_model.summary())

# Use AIC/BIC to determine the model order
print(arma_order_select_ic(diffSeries(phs), max_ar=3, max_ma=5, ic=["aic", "bic"]))

# Use the last `holdout` months as the out-of-sample holdout/validation data set
holdout = 24
train_phs = phs[:-holdout]
test_phs = phs[-holdout:]

# Use the `train_phs` data to fit an ARIMA model.
p = 1
d = 1
q = 1
pre_model_train = ARIMA(train_phs, order=(p, d, q), freq="MS")
model_train = pre_model_train.fit()

# Forecast the values for the times in `test_phs` using the `train_phs` data
forecast = model_train.forecast(steps=holdout)
forecast.name = "Forecast"

# Define a convenience function to find the MAPE error metric
def mape(forecast, actual):
	return (abs(actual - forecast)/actual).mean()

# Find the MAPE values
mape_train = mape(model_train.fittedvalues, train_phs)
mape_test = mape(forecast, test_phs)

# Create a `DataFrame` with the actual values and the forecasted values for the holdout
validate_phs = concat([test_phs, forecast], axis="columns")

# Print the holdout `DataFrame` and the MAPE for both the training and testin data
print("Actual and forecasted PHS:")
print(validate_phs, '\n')
print("MAPE for the training data:", mape_train)
print("MAPE for the testing data:", mape_test, '\n')

# Plot the actual data, fitted values (estimated from `train_phs`), and forecasted values (over the holdout)
plt.plot(phs, color="gray", label="Actual")
plt.plot(model_train.fittedvalues, color="black", linestyle="dashed", label="Fitted")
plt.plot(forecast, color="black", linestyle="dotted", label="Forecasted")
plt.legend()
plt.show()

# Print the summary of the fitted model (including the values of the smoothing constants)
print(model_train.summary())
								</code></pre>
								<h4>SARIMAX Results</h4>
								<table>
									<thead>
										<tr>
											<th colspan="2">Dep. Variable:</th>
											<td colspan="4">PHS</td>
										</tr>
										<tr>
											<th colspan="2">No. Observations:</th>
											<td colspan="4">756</td>
										</tr>
										<tr>
											<th colspan="2">Model:</th>
											<td colspan="4">ARIMA(1, 1, 1)</td>
										</tr>
										<tr>
											<th colspan="2">Log Likelihood:</th>
											<td colspan="4">-4610.091</td>
										</tr>
										<tr>
											<th colspan="2">Date:</th>
											<td colspan="4">Fri, 24 May 2024</td>
										</tr>
										<tr>
											<th colspan="2">AIC:</th>
											<td colspan="4">9226.181</td>
										</tr>
										<tr>
											<th colspan="2">Time:</th>
											<td colspan="4">20:50:07</td>
										</tr>
										<tr>
											<th colspan="2">BIC:</th>
											<td colspan="4">9240.061</td>
										</tr>
										<tr>
											<th colspan="2">Sample:</th>
											<td colspan="4">01-01-1959 - 12-01-2021</td>
										</tr>
										<tr>
											<th colspan="2">HQIC:</th>
											<td colspan="4">9231.528</td>
										</tr>
										<tr>
											<th colspan="2">Covariance Type:</th>
											<td colspan="4">opg</td>
										</tr>
									</thead>
								</table>
								<h4>Parameter Estimates</h4>
								<table>
									<thead>
										<tr>
											<th>Variable</th>
											<th>Coefficient</th>
											<th>Std. Error</th>
											<th>z</th>
											<th>P>|z|</th>
											<th>[0.025</th>
											<th>0.975]</th>
										</tr>
									</thead>
									<tbody>
										<tr>
											<td>ar.L1</td>
											<td>-0.0273</td>
											<td>0.093</td>
											<td>-0.295</td>
											<td>0.768</td>
											<td>-0.209</td>
											<td>0.154</td>
										</tr>
										<tr>
											<td>ma.L1</td>
											<td>-0.3161</td>
											<td>0.088</td>
											<td>-3.604</td>
											<td>0.000</td>
											<td>-0.488</td>
											<td>-0.144</td>
										</tr>
										<tr>
											<td>sigma2</td>
											<td>1.18e+04</td>
											<td>467.224</td>
											<td>25.254</td>
											<td>0.000</td>
											<td>1.09e+04</td>
											<td>1.27e+04</td>
										</tr>
									</tbody>
								</table>
								<h4>Diagnostic Tests</h4>
								<table>

									<tbody>
										<tr>
											<td>Ljung-Box (L1) (Q):</td>
											<td>0.00</td>
										</tr>
										<tr>
											<td>Prob(Q):</td>
											<td>0.98</td>
										</tr>
										<tr>
											<td>Jarque-Bera (JB):</td>
											<td>81.01</td>
										</tr>
										<tr>
											<td>Prob(JB):</td>
											<td>0.00</td>
										</tr>
										<tr>
											<td>Heteroskedasticity (H):</td>
											<td>0.63</td>
										</tr>
										<tr>
											<td>Prob(H) (two-sided):</td>
											<td>0.00</td>
										</tr>
										<tr>
											<td>Skew:</td>
											<td>-0.29</td>
										</tr>
										<tr>
											<td>Kurtosis:</td>
											<td>4.50</td>
										</tr>
									</tbody>
								</table>
								<p>Conclusions on the results:</p>
								<ol>
									<li><b>Goodness of Fit:</b> The various criterion (AIC, BIC, and HQIC) are indicative of a better model.
									The model appears to fit well from the data based on these metrics.</li>
									<li><b>Coefficients:</b> Though the autoregressive term is not significant, the Moving Average term
									suggests it is a very important part of the model.</li>
									<li><b>Variance:</b> It should be noted that variance, however, is quite 
										high, which suggests the model does not cover much of the variance in the data.</li>
									<li><b>Ljung-Box Test</b> <i>(Diagnostic Test)</i><b>:</b> Suggests low autocorrelation, which 
									is great for an ARIMA model.</li>
									<li><b>Jarque-Bera Test</b> <i>(Diagnostic Test)</i><b>:</b> Residuals are not normally distributed,
									meaning that forecasts using this model may not be evenly reliable.</li>
									<li><b>Heteroskedasticity Test</b> <i>(Diagnostic Test)</i><b>:</b> Building upon the previous test, 
									this value suggests that the residuals' variance is inconsistent, which can affect the model's ability 
									to perform.</li>
									<li><b>Skewness and Kurtosis Test</b> <i>(Diagnostic Test)</i><b>:</b> Negative skewness indicates that 
										the residuals are skewed to the left. A kurtosis value greater than 3 indicates heavier tails than 
										a normal distribution, suggesting outliers.</li>
								</ol>
								<p>What we can gather from this overall is that, although the model seems to encapsulate the data well, it is 
									highly likely to falter in its current state if used for forecasting or actual business decisions. Some 
									improvements can be made that can assist in fortifying the model.
								</p>
								<ul>
									<li>Adding additional predictors could address the model not normally covering the patterns in the data.</li>
									<li>Removing the AR term could prove fruitful  as it is seems to be unnecessary. This would also make the 
										model easier to understand if it is not needed.
									</li>
									<li>As data is gathered from a real source, outliers or possibly structural changes in the data source will 
										affect the results.
									</li>
								</ul>
							</ul>
						</div>
						
						<!-- Data Analytics Methods -->
						<div class="project-item">
							<h3>Data Analytics Methods</h3>
							<ul>
								<li><strong>NAIVE Method:</strong> Implemented this simple yet effective forecasting method in Python to provide baseline predictions for time series data. The method was used to compare the performance of more complex models and establish a benchmark for accuracy.
									<pre><code class="language-python">
import numpy as np
import pandas as pd

"""Here we type the UMICS data directly (We are entering the data directly for simplicity.)"""

# The UMICS data
actual = pd.Series([92.0, 91.7, 91.0, 89.0, 94.7, 93.5, 90.0, 89.8, 91.2, 87.2, 93.8, 98.2])

forecast = pd.Series(np.ones(len(actual) + 1))                          # Create the column for the forecast
forecast[0] = float("NaN") # No forecast here

# Run the indented commands repeatedly, with the value of `i` ranging from 1 to `actual.size`
for i in range(1, len(actual)+1):
	forecast[i] = actual[i - 1]                                         # Use the naive method to create the forecast

df = pd.DataFrame(data = {"Actual": actual, "Forecast": forecast})      # Create a DataFrame (like a spreadsheet)
# Print the DataFrame
print(df)

error = df["Actual"] - df["Forecast"]                                   # The error column
absError = abs(error)                                                   # The absolute error column
absPctError = absError/df["Actual"] * 100                               # The absolute percentage error column
mape = absPctError.mean()                                               # Find the mean absolute percentage error

# Append the error, absolute error, and absolute percentage error columns to the DataFrame `df`
df = pd.DataFrame(data = {**df, "Error": error, "Absolute Error": absError, "Absolute % Error": absPctError })
# Print a formatted string (put Python variable names inside of `{}`)
print(f"{df}\n\nMAPE: {mape}")
									</code></pre>
									<!-- Table for Results -->
									<h4>Forecasting Results:</h4>
									<table>
										<thead>
											<tr>
												<th>Actual</th>
												<th>Forecast</th>
												<th>Error</th>
												<th>Absolute Error</th>
												<th>Absolute % Error</th>
											</tr>
										</thead>
										<tbody>
											<tr>
												<td>92.0</td>
												<td>NaN</td>
												<td>NaN</td>
												<td>NaN</td>
												<td>NaN</td>
											</tr>
											<tr>
												<td>91.7</td>
												<td>92.0</td>
												<td>-0.3</td>
												<td>0.3</td>
												<td>0.327154</td>
											</tr>
											<tr>
												<td>91.0</td>
												<td>91.7</td>
												<td>-0.7</td>
												<td>0.7</td>
												<td>0.769231</td>
											</tr>
											<tr>
												<td>89.0</td>
												<td>91.0</td>
												<td>-2.0</td>
												<td>2.0</td>
												<td>2.247191</td>
											</tr>
											<tr>
												<td>94.7</td>
												<td>89.0</td>
												<td>5.7</td>
												<td>5.7</td>
												<td>6.019007</td>
											</tr>
											<tr>
												<td>93.5</td>
												<td>94.7</td>
												<td>-1.2</td>
												<td>1.2</td>
												<td>1.283422</td>
											</tr>
											<tr>
												<td>90.0</td>
												<td>93.5</td>
												<td>-3.5</td>
												<td>3.5</td>
												<td>3.888889</td>
											</tr>
											<tr>
												<td>89.8</td>
												<td>90.0</td>
												<td>-0.2</td>
												<td>0.2</td>
												<td>0.222717</td>
											</tr>
											<tr>
												<td>91.2</td>
												<td>89.8</td>
												<td>1.4</td>
												<td>1.4</td>
												<td>1.535088</td>
											</tr>
											<tr>
												<td>87.2</td>
												<td>91.2</td>
												<td>-4.0</td>
												<td>4.0</td>
												<td>4.587156</td>
											</tr>
											<tr>
												<td>93.8</td>
												<td>87.2</td>
												<td>6.6</td>
												<td>6.6</td>
												<td>7.036247</td>
											</tr>
											<tr>
												<td>98.2</td>
												<td>93.8</td>
												<td>4.4</td>
												<td>4.4</td>
												<td>4.480652</td>
											</tr>
											<tr>
												<td>NaN</td>
												<td>98.2</td>
												<td>NaN</td>
												<td>NaN</td>
												<td>NaN</td>
											</tr>
										</tbody>
									</table>
									<p>MAPE: 2.9451594710015985</p>
								</li>
								<br>
								<li><strong>Causal Regression:</strong> Applied causal regression analysis in Python to model and understand the relationships between dependent and independent variables. This method was utilized to identify key factors influencing specific outcomes and to predict future trends based on causal relationships.
									<pre><code class="language-python">
import numpy as np
from numpy.random import default_rng
from pandas import Series, concat
import statsmodels.api as sm

# Helpful in reproducing results
random_seed_id = 42

def simulate_topping_prices(topping="Ketchup", intercept=1.00, slope=0.042, sigma=0.021, num_weeks=56, rng=None):
	if rng == None:
		rng = default_rng()
	epsilons = rng.normal(loc=0, scale=sigma, size=num_weeks)
	times = np.arange(num_weeks)
	prices = intercept + slope * times + epsilons
	return Series(prices, name=f"{topping} Prices")

def simulate_sandwich_sales(ketchup_prices, mustard_prices, intercept=5.00, coeff_ketchup=0.021, coeff_mustard=0.036, sigma=0.001, rng=None):
	if rng == None:
		rng = default_rng()
	if len(ketchup_prices) != len(mustard_prices):
		return None
	epsilons = rng.normal(loc=0, scale=sigma, size=len(ketchup_prices))
	sales = intercept + coeff_ketchup * ketchup_prices + coeff_mustard * mustard_prices + epsilons
	return Series(sales, name="Sandwich Sales")

rng=default_rng(seed=random_seed_id)
ketchup_prices = simulate_topping_prices(topping="Ketchup", intercept=1.50, slope=0.004, sigma=0.02, rng=rng)
mustard_prices = simulate_topping_prices(topping="Mustard", intercept=2.50, slope=0.008, sigma=0.01, rng=rng)
sandwich_sales = simulate_sandwich_sales(ketchup_prices, mustard_prices, rng=rng)
data=concat([sandwich_sales, ketchup_prices, mustard_prices], axis="columns")

prices_data = data[["Ketchup Prices", "Mustard Prices"]]
print(prices_data.head())

x = sm.add_constant(data[["Ketchup Prices", "Mustard Prices"]])
y = data["Sandwich Sales"]
model = sm.OLS(y, x)
fit = model.fit()
print(fit.summary())

print("The `x` values")
print(x.head())
print("The `y` values")
print(y.head())
									</code></pre>
									<table>
										<tbody>
											<tr>
												<td>Dep. Variable:</td>
												<td>Sandwich Sales</td>
											</tr>
											<tr>
												<td>R-squared:</td>
												<td>0.971</td>
											</tr>
											<tr>
												<td>Model:</td>
												<td>OLS</td>
											</tr>
											<tr>
												<td>Adj. R-squared:</td>
												<td>0.970</td>
											</tr>
											<tr>
												<td>Method:</td>
												<td>Least Squares</td>
											</tr>
											<tr>
												<td>F-statistic:</td>
												<td>899.1</td>
											</tr>
											<tr>
												<td>Date:</td>
												<td>Fri, 24 May 2024</td>
											</tr>
											<tr>
												<td>Prob (F-statistic):</td>
												<td>1.28e-41</td>
											</tr>
											<tr>
												<td>Time:</td>
												<td>18:05:23</td>
											</tr>
											<tr>
												<td>Log-Likelihood:</td>
												<td>305.56</td>
											</tr>
											<tr>
												<td>No. Observations:</td>
												<td>56</td>
											</tr>
											<tr>
												<td>AIC:</td>
												<td>-605.1</td>
											</tr>
											<tr>
												<td>Df Residuals:</td>
												<td>53</td>
											</tr>
											<tr>
												<td>BIC:</td>
												<td>-599.0</td>
											</tr>
											<tr>
												<td>Df Model:</td>
												<td>2</td>
											</tr>
											<tr>
												<td>Covariance Type:</td>
												<td>nonrobust</td>
											</tr>
										</tbody>
										<thead>
											<tr>
												<th colspan="2">Coefficients</th>
											</tr>
										</thead>
										<tbody>
											<tr>
												<td>const</td>
												<td>coef: 5.0007, std err: 0.004, t: 1374.715, P>|t|: 0.000, [0.025: 4.993, 0.975: 5.008]</td>
											</tr>
											<tr>
												<td>Ketchup Prices</td>
												<td>coef: 0.0160, std err: 0.009, t: 1.820, P>|t|: 0.074, [0.025: -0.002, 0.975: 0.034]</td>
											</tr>
											<tr>
												<td>Mustard Prices</td>
												<td>coef: 0.0387, std err: 0.005, t: 8.438, P>|t|: 0.000, [0.025: 0.030, 0.975: 0.048]</td>
											</tr>
										</tbody>
										<thead>
											<tr>
												<th colspan="2">Diagnostics</th>
											</tr>
										</thead>
										<tbody>
											<tr>
												<td>Omnibus:</td>
												<td>4.707</td>
											</tr>
											<tr>
												<td>Prob(Omnibus):</td>
												<td>0.095</td>
											</tr>
											<tr>
												<td>Durbin-Watson:</td>
												<td>1.532</td>
											</tr>
											<tr>
												<td>Jarque-Bera (JB):</td>
												<td>4.090</td>
											</tr>
											<tr>
												<td>Skew:</td>
												<td>0.659</td>
											</tr>
											<tr>
												<td>Prob(JB):</td>
												<td>0.129</td>
											</tr>
											<tr>
												<td>Kurtosis:</td>
												<td>3.121</td>
											</tr>
											<tr>
												<td>Cond. No.:</td>
												<td>235</td>
											</tr>
										</tbody>
									</table>
									<p>To consider the results:</p>
									<ol>
										<li>Coefficients: The intercept of the model is very close to the true intercept of the simulation.
											In addition, the Mustard Price coefficient indicates a strong statistical significance. 
											This is further confirmed by the low p-value.</li>
										<li>R-Squared: The high value of the R-squared statistic indicates good fit by strong capture of the
											variance within the data.
										</li>
										<li>F-stat: A very high F-statistic with a near-zero p-value indicates 
											that the model is statistically significant.</li>
										<li>The estimates are indicated to be rather precise with the small standard errors.</li>
										<li>Diagnostic Tests
											<ul>
												<li>Durbin-Watson: indicates a small degree of positive autocorrelation.</li>
												<li>Omnibus, Jarque-Bera, Skew, and Kurtosis: the residuals might not be perfectly normally distributed.</li>
											</ul>
										</li>
									</ol>
									<p>Overall, the model is a relatively good fit, with notes that there may be a small degree of influence on
										future performance based on past sales prices (small positive autocorrelation). It should be noted as well that
										this model does possess minor issues with residual normality and the previously mentioned autocorrelation.
									</p>
								</li>
								<br>
								<li><strong>ARIMA Models:</strong> Developed ARIMA (AutoRegressive Integrated Moving Average) models using Python to forecast future values in time series data. This approach was employed to analyze historical data and generate predictions, focusing on economic indicators and other trend-sensitive datasets.
									<pre><code class="language-python">
from pandas import DataFrame, read_excel, concat
from matplotlib import pyplot as plt
from statsmodels.tsa.api import acf, pacf, ARIMA, arma_order_select_ic

def correlogramAsDataFrame(correlogram, partial=False):
	# Find the correlogram with confidence intervals for each lag
	if partial:
		label="PACF"
	else:
		label="ACF"
	vals, confints = correlogram
	# Separate the lower bounds and upper bounds of the confidence intervals
	lower = confints.take(indices=0, axis=1)
	upper = confints.take(indices=1, axis=1)

	# Print the correlogram in text form for preciser reading
	return DataFrame({label: vals, "Lower": lower, "Upper": upper})

def plotCorrelogram(correlogram):
	# Plot the correlogram with the confidence intervals for each lag
	plt.plot(correlogram.iloc[:,0], color="gray", label=correlogram.columns[0])
	plt.plot(correlogram[["Lower"]], color="black", linestyle="dashed", label="Lower")
	plt.plot(correlogram[["Upper"]], color="black", linestyle="dotted", label="Upper")
	plt.legend()
	plt.show()

def diffSeries(series):
	return [series[i+1]-series[i] for i in range(len(series) - 1)]

# Access the data set; `sheet_name=None` means import all sheets from the workbook.
data=read_excel("https://www.census.gov/construction/nrc/xls/starts_cust.xlsx", sheet_name=None, header=[5, 5, 5, 5])

# Extract rows 0 to 779 and columns 0 to 1 from the "Seasonally Adjusted" sheet.
df = data["Seasonally Adjusted"].iloc[0:780, 0:2]
# Update the column headers
df.columns = ["Month", "PHS"]
# Get the phs `Series` by itself
phs = df["PHS"]
# Label the rows of `phs` by the month
phs.index = df["Month"]

# Do an initial exploratory plot
plt.plot(phs)
plt.show()

# Find and display the autocorrelation function.
correlAcf = acf(phs, nlags=10, alpha=0.05)
acfDF = correlogramAsDataFrame(correlAcf)
plotCorrelogram(acfDF)

# Find and display the partial autocorrelation function.
correlPacf = pacf(phs, nlags=10, alpha=0.05)
pacfDF = correlogramAsDataFrame(correlPacf, partial=True)
plotCorrelogram(pacfDF)

# Order selection
p = 1
d = 1
q = 2
model = ARIMA(phs, order=(p, d, q))
fitted_model = model.fit()
print(fitted_model.summary())

# Use AIC/BIC to determine the model order
print(arma_order_select_ic(diffSeries(phs), max_ar=3, max_ma=5, ic=["aic", "bic"]))

# Use the last `holdout` months as the out-of-sample holdout/validation data set
holdout = 24
train_phs = phs[:-holdout]
test_phs = phs[-holdout:]

# Use the `train_phs` data to fit an ARIMA model.
p = 1
d = 1
q = 1
pre_model_train = ARIMA(train_phs, order=(p, d, q), freq="MS")
model_train = pre_model_train.fit()

# Forecast the values for the times in `test_phs` using the `train_phs` data
forecast = model_train.forecast(steps=holdout)
forecast.name = "Forecast"

# Define a convenience function to find the MAPE error metric
def mape(forecast, actual):
	return (abs(actual - forecast)/actual).mean()

# Find the MAPE values
mape_train = mape(model_train.fittedvalues, train_phs)
mape_test = mape(forecast, test_phs)

# Create a `DataFrame` with the actual values and the forecasted values for the holdout
validate_phs = concat([test_phs, forecast], axis="columns")

# Print the holdout `DataFrame` and the MAPE for both the training and testin data
print("Actual and forecasted PHS:")
print(validate_phs, '\n')
print("MAPE for the training data:", mape_train)
print("MAPE for the testing data:", mape_test, '\n')

# Plot the actual data, fitted values (estimated from `train_phs`), and forecasted values (over the holdout)
plt.plot(phs, color="gray", label="Actual")
plt.plot(model_train.fittedvalues, color="black", linestyle="dashed", label="Fitted")
plt.plot(forecast, color="black", linestyle="dotted", label="Forecasted")
plt.legend()
plt.show()

# Print the summary of the fitted model (including the values of the smoothing constants)
print(model_train.summary())
									</code></pre>
									<h4>SARIMAX Results</h4>
									<table>
										<thead>
											<tr>
												<th colspan="2">Dep. Variable:</th>
												<td colspan="4">PHS</td>
											</tr>
											<tr>
												<th colspan="2">No. Observations:</th>
												<td colspan="4">756</td>
											</tr>
											<tr>
												<th colspan="2">Model:</th>
												<td colspan="4">ARIMA(1, 1, 1)</td>
											</tr>
											<tr>
												<th colspan="2">Log Likelihood:</th>
												<td colspan="4">-4610.091</td>
											</tr>
											<tr>
												<th colspan="2">Date:</th>
												<td colspan="4">Fri, 24 May 2024</td>
											</tr>
											<tr>
												<th colspan="2">AIC:</th>
												<td colspan="4">9226.181</td>
											</tr>
											<tr>
												<th colspan="2">Time:</th>
												<td colspan="4">20:50:07</td>
											</tr>
											<tr>
												<th colspan="2">BIC:</th>
												<td colspan="4">9240.061</td>
											</tr>
											<tr>
												<th colspan="2">Sample:</th>
												<td colspan="4">01-01-1959 - 12-01-2021</td>
											</tr>
											<tr>
												<th colspan="2">HQIC:</th>
												<td colspan="4">9231.528</td>
											</tr>
											<tr>
												<th colspan="2">Covariance Type:</th>
												<td colspan="4">opg</td>
											</tr>
										</thead>
									</table>
									<h4>Parameter Estimates</h4>
									<table>
										<thead>
											<tr>
												<th>Variable</th>
												<th>Coefficient</th>
												<th>Std. Error</th>
												<th>z</th>
												<th>P>|z|</th>
												<th>[0.025</th>
												<th>0.975]</th>
											</tr>
										</thead>
										<tbody>
											<tr>
												<td>ar.L1</td>
												<td>-0.0273</td>
												<td>0.093</td>
												<td>-0.295</td>
												<td>0.768</td>
												<td>-0.209</td>
												<td>0.154</td>
											</tr>
											<tr>
												<td>ma.L1</td>
												<td>-0.3161</td>
												<td>0.088</td>
												<td>-3.604</td>
												<td>0.000</td>
												<td>-0.488</td>
												<td>-0.144</td>
											</tr>
											<tr>
												<td>sigma2</td>
												<td>1.18e+04</td>
												<td>467.224</td>
												<td>25.254</td>
												<td>0.000</td>
												<td>1.09e+04</td>
												<td>1.27e+04</td>
											</tr>
										</tbody>
									</table>
									<h4>Diagnostic Tests</h4>
									<table>

										<tbody>
											<tr>
												<td>Ljung-Box (L1) (Q):</td>
												<td>0.00</td>
											</tr>
											<tr>
												<td>Prob(Q):</td>
												<td>0.98</td>
											</tr>
											<tr>
												<td>Jarque-Bera (JB):</td>
												<td>81.01</td>
											</tr>
											<tr>
												<td>Prob(JB):</td>
												<td>0.00</td>
											</tr>
											<tr>
												<td>Heteroskedasticity (H):</td>
												<td>0.63</td>
											</tr>
											<tr>
												<td>Prob(H) (two-sided):</td>
												<td>0.00</td>
											</tr>
											<tr>
												<td>Skew:</td>
												<td>-0.29</td>
											</tr>
											<tr>
												<td>Kurtosis:</td>
												<td>4.50</td>
											</tr>
										</tbody>
									</table>
									<p>Conclusions on the results:</p>
									<ol>
										<li><b>Goodness of Fit:</b> The various criterion (AIC, BIC, and HQIC) are indicative of a better model.
										The model appears to fit well from the data based on these metrics.</li>
										<li><b>Coefficients:</b> Though the autoregressive term is not significant, the Moving Average term
										suggests it is a very important part of the model.</li>
										<li><b>Variance:</b> It should be noted that variance, however, is quite 
											high, which suggests the model does not cover much of the variance in the data.</li>
										<li><b>Ljung-Box Test</b> <i>(Diagnostic Test)</i><b>:</b> Suggests low autocorrelation, which 
										is great for an ARIMA model.</li>
										<li><b>Jarque-Bera Test</b> <i>(Diagnostic Test)</i><b>:</b> Residuals are not normally distributed,
										meaning that forecasts using this model may not be evenly reliable.</li>
										<li><b>Heteroskedasticity Test</b> <i>(Diagnostic Test)</i><b>:</b> Building upon the previous test, 
										this value suggests that the residuals' variance is inconsistent, which can affect the model's ability 
										to perform.</li>
										<li><b>Skewness and Kurtosis Test</b> <i>(Diagnostic Test)</i><b>:</b> Negative skewness indicates that 
											the residuals are skewed to the left. A kurtosis value greater than 3 indicates heavier tails than 
											a normal distribution, suggesting outliers.</li>
									</ol>
									<p>What we can gather from this overall is that, although the model seems to encapsulate the data well, it is 
										highly likely to falter in its current state if used for forecasting or actual business decisions. Some 
										improvements can be made that can assist in fortifying the model.
									</p>
									<ul>
										<li>Adding additional predictors could address the model not normally covering the patterns in the data.</li>
										<li>Removing the AR term could prove fruitful  as it is seems to be unnecessary. This would also make the 
											model easier to understand if it is not needed.
										</li>
										<li>As data is gathered from a real source, outliers or possibly structural changes in the data source will 
											affect the results.
										</li>
									</ul>
								</li>
							</ul>
						</div>
					</div>

					<div id="tab4" class="tab-content">
						<!-- Personal Projects -->
						
						<!-- YouTube Data Pipeline -->
						<div class="project-item">
							<h3>YouTube Data Pipeline</h3>
							<ul>
								<li>Set up a miniature data pipeline using MySQL Server and YouTube API to gather video data and metadata from Youtube.</li>
								<li>Utilized shell scripts and Python data analysis to determine trends in data and automatically update on a weekly basis.</li>
							</ul>
						</div>
						
						<!-- Steam Google Link Task Manager -->
						<div class="project-item">
							<h3>Steam Google Link Task Manager</h3>
							<ul>
								<li>Utilized Steam's API to create a Google Workspace that links to a large STEAM database and queries for records associated with user accounts.</li>
								<li>Developed a custom Appscript (JS) that gathers, dedupes, and reorganizes records into tasks visible only to the appropriate user.</li>
								<li>Enabled users to mark task progress and completion, with interactive boards responding to input, archiving old tasks, and color-coding priorities.</li>
								<li>Tasks sync between users when appropriate, with updates running daily on Google servers, efficiently processing over 1,000 records per user.</li>
							</ul>
						</div>
					</div>
				</div>
			</section>
		</div>

		<div class="content-container">
			<section id="other-experiences">
				<div class="container">
					<h2>Other Experiences</h2>
					
					<!-- Music Composition Entry -->
					<div class="experience-item">
						<h3>Music Composer</h3>
						<p><strong>Freelance</strong> – January 2016 – Present</p>
						<ul>
							<li>Composed original music pieces across various genres, including classical, electronic, and ambient.</li>
							<li>Collaborated with artists, filmmakers, and game developers to create custom soundtracks and scores.</li>
							<li>Produced and mixed tracks using industry-standard software and techniques.</li>
							<li>Gained recognition through a growing portfolio and multiple projects on SoundCloud.</li>
						</ul>
						<p>Listen to my work on <a href="https://soundcloud.com/naryan-aggarwal" target="_blank" rel="noopener noreferrer">SoundCloud</a>.</p>
					</div>
					
					 <!-- Innovation DuPage Incubator Project -->
					<div class="experience-item">
						<h3>Student Participant</h3>
						<p>Innovation DuPage Incubator, Lombard, IL | August 2018 – June 2019</p>
						<ul>
							<li>Founded a private sports accessory company as the CEO.</li>
							<li>Presented in a shark tank to investors, earning 2nd place among 7 companies.</li>
						</ul>
					</div>

					<!-- Carnegie Mellon University AI Research Program Project -->
					<div class="experience-item">
						<h3>Student Participant</h3>
						<p>Carnegie Mellon University – Precollege AI Research Program, Pittsburgh, PA | July 2018</p>
						<ul>
							<li>Rebuilt from scratch the Anki Cosmo SDK for the Cosmo robotic companion.</li>
							<li>Researched intuitive decision-making in robotics using the Cosmo AI.</li>
						</ul>
					</div>

					<!-- GSC Technology Company Project -->
					<div class="experience-item">
						<h3>Student Participant</h3>
						<p>GSC Technology Company, Oakbrook, IL | August 2017</p>
						<ul>
							<li>Attended a 48-hour professional training course in mechanical 3D modeling using Solidworks.</li>
							<li>Created dynamic and functional digital prototype models for 5+ mechanical devices.</li>
							<li>Learned optimization techniques for 3D printing with mobile parts.</li>
						</ul>
					</div>
				</div>
			</section>
		</div>

		<div class="content-container">
			<!-- Skills Section -->
			<section id="skills-section">
				<h2>Skills</h2>
				<div class="skills-container">
					<!-- Technical Skills -->
					<div class="skills-category">
						<h3>Technical Skills</h3>
						<ul>
							<li><strong>Programming Languages:</strong> Python, SQL, Bash, HTML, CSS, JavaScript, Java</li>
							<li><strong>Data Analysis:</strong> NAIVE Method, Causal Regression, Multilinear Regression, ARIMA</li>
							<li><strong>Tools & Technologies:</strong> Excel, MySQL, Redshift, AWS, Google Workspace, Steam API</li>
							<li><strong>Software:</strong> Adobe Photoshop, Adobe Illustrator, Blender, Cinema4D</li>
							<li><strong>Data Visualization:</strong> Matplotlib, Seaborn, Tableau</li>
							<li><strong>Fine Arts:</strong> Music Composition, Animation, Drawing, FLStudio, FincalCut, DaVinci Resolve, Photoshop, Krita</li>
						</ul>
					</div>

					<!-- Soft Skills -->
					<div class="skills-category">
						<h3>Soft Skills</h3>
						<ul>
							<li><strong>Analytical Thinking:</strong> Proven ability to analyze complex datasets and provide actionable insights.</li>
							<li><strong>Project Management:</strong> Experience in leading projects, organizing tasks, and managing deadlines.</li>
							<li><strong>Communication:</strong> Effective communicator with experience in client interactions and team collaborations.</li>
							<li><strong>Relationship Management:</strong> Nurturing long-term business relationships to foster mutual growth.</li>
							<li><strong>Problem-Solving:</strong> Strong problem-solving skills demonstrated through various projects and internships.</li>
							<li><strong>Adaptability:</strong> Ability to quickly learn new technologies and adapt to changing project requirements.</li>
							<li><strong>Consultative Selling:</strong> Providing solutions that align with the specific needs of clients.</li>
						</ul>
					</div>
				</div>
			</section>
		</div>
		
		<div class="content-container">
			<!-- Contact Section -->
			<section id="contact">
				<div class="container">
					<h2>Contact</h2>
					<p>If you would like to get in touch, please feel free to email me at <a href="mailto:naryantaggarwal@gmail.com">naryantaggarwal@gmail.com</a> or connect with me on <a href="#">LinkedIn</a>.</p>
				</div>
			</section>

			<!-- Footer -->
			<footer>
				<div class="container">
					<p>&copy; 2025 Naryan Aggarwal. All Rights Reserved.</p>
				</div>
			</footer>
		</div>
	</div>
	
	<script src="scripts.js"></script>

</body>

</html>
